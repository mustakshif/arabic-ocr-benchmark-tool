#!/usr/bin/env python3
"""
OCR Model Benchmark Script
===========================
å¯¹æ¯”æµ‹è¯• Gemini / Mistral OCR æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
1. å°†æµ‹è¯•PDFæ”¾å…¥ samples/ ç›®å½•
2. å°†å¯¹åº”çš„äººå·¥æ ¡å¯¹æ–‡æœ¬æ”¾å…¥ ground_truth/ ç›®å½• (æ–‡ä»¶åéœ€å¯¹åº”)
   âš ï¸ é‡è¦ï¼šground_truthæ–‡ä»¶éœ€è¦ç”¨ "---" åˆ†éš”æ¯ä¸€é¡µçš„å†…å®¹ï¼Œä¾‹å¦‚ï¼š
   
   ç¬¬ä¸€é¡µå†…å®¹...
   ---
   ç¬¬äºŒé¡µå†…å®¹...
   ---
   ç¬¬ä¸‰é¡µå†…å®¹...
   
3. å¤åˆ¶ config.example.env ä¸º .env å¹¶å¡«å…¥APIå¯†é’¥
4. è¿è¡Œ: python benchmark_test.py
5. æŸ¥çœ‹ results/ ç›®å½•çš„æµ‹è¯•æŠ¥å‘Š
"""

import os
import sys
import json
import time
import base64
import difflib
import argparse
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field, asdict
from typing import Optional, List, Dict, Tuple
from collections import defaultdict

try:
    from google import genai
    from google.genai import types
    from mistralai import Mistral
    from pdf2image import convert_from_path
    from PIL import Image
    from dotenv import load_dotenv
    from tqdm import tqdm
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–: {e}")
    print("è¯·è¿è¡Œ: pip install -r requirements.txt")
    sys.exit(1)


@dataclass
class BenchmarkConfig:
    gemini_api_key: str = ""
    mistral_api_key: str = ""
    samples_dir: Path = field(default_factory=lambda: Path("samples"))
    ground_truth_dir: Path = field(default_factory=lambda: Path("ground_truth"))
    results_dir: Path = field(default_factory=lambda: Path("results"))
    pdf_dpi: int = 200
    primary_language: str = "Arabic"
    
    @classmethod
    def from_env(cls, base_dir: Path) -> "BenchmarkConfig":
        load_dotenv(base_dir / ".env")
        return cls(
            gemini_api_key=os.getenv("GEMINI_API_KEY", ""),
            mistral_api_key=os.getenv("MISTRAL_API_KEY", ""),
            samples_dir=base_dir / "samples",
            ground_truth_dir=base_dir / "ground_truth",
            results_dir=base_dir / "results",
            pdf_dpi=int(os.getenv("PDF_DPI", "200")),
            primary_language=os.getenv("PRIMARY_LANGUAGE", "Arabic")
        )


@dataclass
class OCRResult:
    model_name: str
    sample_name: str
    page_num: int
    ocr_text: str
    ground_truth: str
    cer: float  # Character Error Rate
    wer: float  # Word Error Rate
    processing_time: float
    cost_estimate: float
    error: Optional[str] = None


def calculate_cer(ocr_text: str, ground_truth: str) -> float:
    """Character Error Rate = Levenshteinè·ç¦» / ground_truthé•¿åº¦"""
    if not ground_truth:
        return 1.0 if ocr_text else 0.0
    
    matcher = difflib.SequenceMatcher(None, ground_truth, ocr_text)
    distance = sum(
        max(i2 - i1, j2 - j1) 
        for tag, i1, i2, j1, j2 in matcher.get_opcodes() 
        if tag != 'equal'
    )
    return min(distance / len(ground_truth), 1.0)


def calculate_wer(ocr_text: str, ground_truth: str) -> float:
    """Word Error Rate"""
    gt_words = ground_truth.split()
    ocr_words = ocr_text.split()
    
    if not gt_words:
        return 1.0 if ocr_words else 0.0
    
    matcher = difflib.SequenceMatcher(None, gt_words, ocr_words)
    distance = sum(
        max(i2 - i1, j2 - j1) 
        for tag, i1, i2, j1, j2 in matcher.get_opcodes() 
        if tag != 'equal'
    )
    return min(distance / len(gt_words), 1.0)


def image_to_base64(image: Image.Image) -> str:
    import io
    buffer = io.BytesIO()
    image.save(buffer, format='PNG')
    return base64.b64encode(buffer.getvalue()).decode('utf-8')


class GeminiTester:
    # Pricing as of Jan 2026 (Google AI Studio / Vertex AI standard rates, context â‰¤200K)
    # Pro and Gemini-3 models have thinking tokens billed separately
    # Source: https://cloud.google.com/vertex-ai/generative-ai/pricing
    MODELS = {
        "gemini-3-pro-preview": {
            "name": "gemini-3-pro-preview",
            "input_cost_per_million": 2.00,
            "output_cost_per_million": 6.00,
            "thinking_cost_per_million": 1.00
        },
        "gemini-3-flash-preview": {
            "name": "gemini-3-flash-preview",
            "input_cost_per_million": 0.50,
            "output_cost_per_million": 1.50,
            "thinking_cost_per_million": 0.30
        },
        "gemini-2.5-pro": {
            "name": "gemini-2.5-pro",
            "input_cost_per_million": 1.25,
            "output_cost_per_million": 5.00,
            "thinking_cost_per_million": 1.00
        },
        "gemini-2.5-flash": {
            "name": "gemini-2.5-flash",
            "input_cost_per_million": 0.30,
            "output_cost_per_million": 1.00
        }
    }

    def __init__(self, api_key: str, language: str = "Arabic"):
        self.client = genai.Client(api_key=api_key)
        self.language = language
        self.prompt = self._build_prompt()

    def _build_prompt(self) -> str:
        return f"""OCR this document page. Output the exact text in {self.language}, preserving structure.
Use Markdown formatting. Do not translate. Output text only."""

    def ocr_image(self, image: Image.Image, model_key: str) -> Tuple[str, float, float]:
        model_info = self.MODELS[model_key]
        
        import io
        buffer = io.BytesIO()
        image.save(buffer, format='PNG')
        image_bytes = buffer.getvalue()

        start_time = time.time()
        response = self.client.models.generate_content(
            model=model_info["name"],
            contents=[
                types.Content(
                    role="user",
                    parts=[
                        types.Part(text=self.prompt),
                        types.Part.from_bytes(data=image_bytes, mime_type="image/png")
                    ]
                )
            ]
        )
        elapsed = time.time() - start_time

        text = response.text.strip() if response.text else ""

        usage_meta = {}
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage_meta = response.usage_metadata
        
        input_tokens = getattr(usage_meta, 'prompt_token_count', None)
        output_tokens = getattr(usage_meta, 'candidates_token_count', None)
        thinking_tokens = getattr(usage_meta, 'thoughts_token_count', 0) or 0
        
        if input_tokens is None:
            image_tokens = self._estimate_image_tokens(image)
            input_tokens = image_tokens + 50
        if output_tokens is None:
            output_tokens = len(text) // 4
        
        cost = (input_tokens * model_info["input_cost_per_million"] +
                output_tokens * model_info["output_cost_per_million"]) / 1_000_000
        
        if thinking_tokens > 0 and "thinking_cost_per_million" in model_info:
            cost += (thinking_tokens * model_info["thinking_cost_per_million"]) / 1_000_000

        return text, elapsed, cost

    def _estimate_image_tokens(self, image: Image.Image) -> int:
        """Estimate token count for an image based on its dimensions.
        
        Google charges ~258 tokens for a 1024x1024 image.
        For larger images, tokens scale proportionally.
        """
        width, height = image.size
        pixels = width * height
        base_pixels = 1024 * 1024
        base_tokens = 258
        
        estimated_tokens = int((pixels / base_pixels) * base_tokens)
        return max(estimated_tokens, base_tokens)


class MistralTester:
    MODELS = {
        "mistral-ocr-latest": {
            "name": "mistral-ocr-latest",
            "cost_per_page": 0.002
        }
    }
    
    def __init__(self, api_key: str):
        self.client = Mistral(api_key=api_key)
        
    def ocr_image(self, image: Image.Image, model_key: str = "mistral-ocr-latest") -> Tuple[str, float, float]:
        model_info = self.MODELS[model_key]
        img_b64 = image_to_base64(image)
        
        start_time = time.time()
        
        response = self.client.ocr.process(
            model=model_info["name"],
            document={
                "type": "image_url",
                "image_url": f"data:image/png;base64,{img_b64}"
            }
        )
        elapsed = time.time() - start_time
        
        text = ""
        if hasattr(response, 'pages') and response.pages:
            for page in response.pages:
                if hasattr(page, 'markdown'):
                    text += page.markdown + "\n"
        
        return text.strip(), elapsed, model_info["cost_per_page"]


def run_benchmark(config: BenchmarkConfig, models_to_test: List[str]) -> List[OCRResult]:
    results: List[OCRResult] = []
    
    gemini_models = [m for m in models_to_test if m.startswith("gemini")]
    mistral_models = [m for m in models_to_test if m.startswith("mistral")]
    
    gemini_tester = GeminiTester(config.gemini_api_key, config.primary_language) if gemini_models and config.gemini_api_key else None
    mistral_tester = MistralTester(config.mistral_api_key) if mistral_models and config.mistral_api_key else None
    
    sample_files = list(config.samples_dir.glob("*.pdf"))
    
    if not sample_files:
        print(f"æœªæ‰¾åˆ°æ ·æœ¬PDFï¼Œè¯·å°†æµ‹è¯•æ–‡ä»¶æ”¾å…¥ {config.samples_dir}")
        return results
    
    print(f"\næ‰¾åˆ° {len(sample_files)} ä¸ªæµ‹è¯•æ ·æœ¬")
    print("=" * 60)
    
    for pdf_path in sample_files:
        sample_name = pdf_path.stem
        
        gt_path = config.ground_truth_dir / f"{sample_name}.txt"
        if not gt_path.exists():
            print(f"âš ï¸  è·³è¿‡ {sample_name}: æœªæ‰¾åˆ° ground_truth/{sample_name}.txt")
            continue
        
        with open(gt_path, 'r', encoding='utf-8') as f:
            ground_truth_full = f.read().strip()
        
        print(f"\nğŸ“„ æµ‹è¯•æ ·æœ¬: {sample_name}")
        
        images = convert_from_path(pdf_path, dpi=config.pdf_dpi)
        num_pages = len(images)
        print(f"   å…± {num_pages} é¡µ")
        
        gt_pages = ground_truth_full.split("\n---\n") if "\n---\n" in ground_truth_full else None
        
        if gt_pages and len(gt_pages) != num_pages:
            print(f"   âš ï¸ è­¦å‘Š: ground_truth æœ‰ {len(gt_pages)} é¡µï¼ŒPDFæœ‰ {num_pages} é¡µï¼Œé¡µæ•°ä¸åŒ¹é…")
        
        if not gt_pages:
            print(f"   âš ï¸ è­¦å‘Š: ground_truth æœªæŒ‰é¡µåˆ†éš” (ç”¨ --- åˆ†éš”)ï¼Œå°†ä½¿ç”¨æ•´æ–‡æ¡£å¯¹æ¯”")
        
        for page_num, image in enumerate(images, 1):
            page_gt = gt_pages[page_num - 1].strip() if gt_pages and page_num <= len(gt_pages) else ""
            
            if gemini_tester:
                for model_key in gemini_models:
                    try:
                        print(f"   [{model_key}] é¡µ {page_num}...", end=" ", flush=True)
                        text, elapsed, cost = gemini_tester.ocr_image(image, model_key)
                        
                        if page_gt:
                            cer = calculate_cer(text, page_gt)
                            wer = calculate_wer(text, page_gt)
                            print(f"CER={cer:.2%}, WER={wer:.2%}, {elapsed:.1f}s")
                        else:
                            cer, wer = 0.0, 0.0
                            print(f"å®Œæˆ ({elapsed:.1f}s)")
                        
                        results.append(OCRResult(
                            model_name=model_key,
                            sample_name=sample_name,
                            page_num=page_num,
                            ocr_text=text,
                            ground_truth=page_gt,
                            cer=cer,
                            wer=wer,
                            processing_time=elapsed,
                            cost_estimate=cost
                        ))
                        time.sleep(1)
                    except Exception as e:
                        print(f"é”™è¯¯: {e}")
                        results.append(OCRResult(
                            model_name=model_key,
                            sample_name=sample_name,
                            page_num=page_num,
                            ocr_text="",
                            ground_truth=page_gt,
                            cer=1.0,
                            wer=1.0,
                            processing_time=0,
                            cost_estimate=0,
                            error=str(e)
                        ))
            
            if mistral_tester:
                for model_key in mistral_models:
                    try:
                        print(f"   [{model_key}] é¡µ {page_num}...", end=" ", flush=True)
                        text, elapsed, cost = mistral_tester.ocr_image(image, model_key)
                        
                        if page_gt:
                            cer = calculate_cer(text, page_gt)
                            wer = calculate_wer(text, page_gt)
                            print(f"CER={cer:.2%}, WER={wer:.2%}, {elapsed:.1f}s")
                        else:
                            cer, wer = 0.0, 0.0
                            print(f"å®Œæˆ ({elapsed:.1f}s)")
                        
                        results.append(OCRResult(
                            model_name=model_key,
                            sample_name=sample_name,
                            page_num=page_num,
                            ocr_text=text,
                            ground_truth=page_gt,
                            cer=cer,
                            wer=wer,
                            processing_time=elapsed,
                            cost_estimate=cost
                        ))
                        time.sleep(1)
                    except Exception as e:
                        print(f"é”™è¯¯: {e}")
                        results.append(OCRResult(
                            model_name=model_key,
                            sample_name=sample_name,
                            page_num=page_num,
                            ocr_text="",
                            ground_truth=page_gt,
                            cer=1.0,
                            wer=1.0,
                            processing_time=0,
                            cost_estimate=0,
                            error=str(e)
                        ))
    
    return results


def generate_report(results: List[OCRResult], output_dir: Path) -> Path:
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = output_dir / f"benchmark_report_{timestamp}.md"
    
    stats: Dict[str, Dict] = defaultdict(lambda: {
        "total_cer": 0.0,
        "total_wer": 0.0,
        "total_time": 0.0,
        "total_cost": 0.0,
        "count": 0,
        "errors": 0
    })
    
    for r in results:
        s = stats[r.model_name]
        if r.error:
            s["errors"] += 1
        else:
            s["total_cer"] += r.cer
            s["total_wer"] += r.wer
            s["total_time"] += r.processing_time
            s["total_cost"] += r.cost_estimate
            s["count"] += 1
    
    report_lines = [
        "# OCR Benchmark Report",
        f"\n*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
        "## Summary\n",
        "| Model | Avg CER â†“ | Avg WER â†“ | Avg Time | Est. Cost/Page | Pages | Errors |",
        "|-------|-----------|-----------|----------|----------------|-------|--------|"
    ]
    
    for model_name, s in sorted(stats.items()):
        if s["count"] > 0:
            avg_cer = s["total_cer"] / s["count"]
            avg_wer = s["total_wer"] / s["count"]
            avg_time = s["total_time"] / s["count"]
            avg_cost = s["total_cost"] / s["count"]
            report_lines.append(
                f"| {model_name} | {avg_cer:.2%} | {avg_wer:.2%} | "
                f"{avg_time:.1f}s | ${avg_cost:.4f} | {s['count']} | {s['errors']} |"
            )
    
    report_lines.extend([
        "\n## Metrics Explanation\n",
        "- **CER (Character Error Rate)**: å­—ç¬¦çº§é”™è¯¯ç‡ï¼Œè¶Šä½è¶Šå¥½",
        "- **WER (Word Error Rate)**: è¯çº§é”™è¯¯ç‡ï¼Œè¶Šä½è¶Šå¥½",
        "- **Est. Cost**: ä¼°ç®—æˆæœ¬ï¼ŒåŸºäºå®˜æ–¹å®šä»·\n",
        "## Detailed Results\n"
    ])
    
    for r in results:
        status = "âŒ ERROR" if r.error else f"CER={r.cer:.2%}"
        report_lines.append(f"- **{r.model_name}** | {r.sample_name} p{r.page_num} | {status}")
    
    report_lines.extend([
        "\n## Recommendation\n"
    ])
    
    valid_stats = {k: v for k, v in stats.items() if v["count"] > 0}
    if valid_stats:
        best_model = min(valid_stats.items(), key=lambda x: x[1]["total_cer"] / x[1]["count"])
        report_lines.append(f"åŸºäºæµ‹è¯•ç»“æœï¼Œ**{best_model[0]}** åœ¨å‡†ç¡®ç‡ä¸Šè¡¨ç°æœ€ä½³ã€‚")
    
    report_content = "\n".join(report_lines)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    json_path = output_dir / f"benchmark_raw_{timestamp}.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump([asdict(r) for r in results], f, ensure_ascii=False, indent=2)
    
    return report_path


ALL_GEMINI_MODELS = ["gemini-3-pro-preview", "gemini-3-flash-preview", "gemini-2.5-pro", "gemini-2.5-flash"]
ALL_MISTRAL_MODELS = ["mistral-ocr-latest"]

def main():
    parser = argparse.ArgumentParser(
        description="OCR Model Benchmark",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python benchmark_test.py                           # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
  python benchmark_test.py --models gemini-3-pro-preview gemini-3-flash-preview
  python benchmark_test.py --models mistral-ocr-latest
  python benchmark_test.py --gemini-only             # æµ‹è¯•æ‰€æœ‰Geminiæ¨¡å‹
  python benchmark_test.py --mistral-only            # æµ‹è¯•æ‰€æœ‰Mistralæ¨¡å‹

å¯ç”¨æ¨¡å‹:
  Gemini: gemini-3-pro-preview, gemini-3-flash-preview, gemini-2.5-pro, gemini-2.5-flash
  Mistral: mistral-ocr-latest
        """
    )
    parser.add_argument("--models", nargs="+", help="æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨")
    parser.add_argument("--gemini-only", action="store_true", help="åªæµ‹è¯•Geminiæ¨¡å‹")
    parser.add_argument("--mistral-only", action="store_true", help="åªæµ‹è¯•Mistralæ¨¡å‹")
    args = parser.parse_args()
    
    base_dir = Path(__file__).parent
    config = BenchmarkConfig.from_env(base_dir)
    
    config.samples_dir.mkdir(parents=True, exist_ok=True)
    config.ground_truth_dir.mkdir(parents=True, exist_ok=True)
    config.results_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("OCR Model Benchmark")
    print("=" * 60)
    
    if args.models:
        models_to_test = args.models
    elif args.mistral_only:
        models_to_test = ALL_MISTRAL_MODELS
    elif args.gemini_only:
        models_to_test = ALL_GEMINI_MODELS
    else:
        models_to_test = ALL_GEMINI_MODELS + ALL_MISTRAL_MODELS
    
    gemini_models = [m for m in models_to_test if m.startswith("gemini")]
    mistral_models = [m for m in models_to_test if m.startswith("mistral")]
    
    if gemini_models and not config.gemini_api_key:
        print(f"âš ï¸  è·³è¿‡ Gemini æ¨¡å‹: GEMINI_API_KEY æœªé…ç½®")
        models_to_test = [m for m in models_to_test if not m.startswith("gemini")]
    
    if mistral_models and not config.mistral_api_key:
        print(f"âš ï¸  è·³è¿‡ Mistral æ¨¡å‹: MISTRAL_API_KEY æœªé…ç½®")
        models_to_test = [m for m in models_to_test if not m.startswith("mistral")]
    
    if not models_to_test:
        print("âŒ æ²¡æœ‰å¯æµ‹è¯•çš„æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„APIå¯†é’¥")
        sys.exit(1)
    
    print(f"æµ‹è¯•æ¨¡å‹: {', '.join(models_to_test)}")
    print(f"æµ‹è¯•è¯­è¨€: {config.primary_language}")
    
    results = run_benchmark(config, models_to_test)
    
    if results:
        report_path = generate_report(results, config.results_dir)
        print("\n" + "=" * 60)
        print("æµ‹è¯•å®Œæˆ!")
        print(f"æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print("=" * 60)
        
        with open(report_path, 'r', encoding='utf-8') as f:
            print("\n" + f.read())
    else:
        print("\næœªäº§ç”Ÿä»»ä½•ç»“æœï¼Œè¯·æ£€æŸ¥æ ·æœ¬æ–‡ä»¶å’Œé…ç½®")


if __name__ == "__main__":
    main()
